# Как я всё это развернул

## Самые запоминающиеся проблемы и их решения
Ниже будут описаны проблемы при инсталляции, которые запомнились больше всего и их решения

### Бинарный файл bingo выводил Hello, world

Решение:
Запустить бинарный файл с флагом --help

### Бинарный файл не запускает сервер

Решение:
Смотрим стандартный конфиг, который предлагает приложение.
Создаем и записываем свои данные для базы данных и почту.

Бинарник просто так конфиг не увидит, нужно было понять в каком каталоге бинарный файл ожидает увидеть конфиг.

С помощью strace я определил что приложение ищет конфиг в папке `/opt/bingo`.

После того, как я подготовил базу данных локально и указал адрес в конфиге можно пробовать запускать `./bingo prepare_db`

Подготовка базы занимает порядка получаса, после этого можно пробовать стартовать сервер

Вылетает еще одна ошибка. алгоритм такой же:
смотрим системные вызовы бинарника с помощью strace и определяем, что также для запуска требуется создать каталог для логов `/opt/bongo/logs/fa2acf5dcc`

### Не понятно на каком порту работает сервис

С помощью команды `netstat -tulnp | grep bingo` я определил порт приложения: 30238

### Как ускорять отдачу HTTP-запросов

Проверяя таймауты я обнаружил, что эндпоинт /api/session отдаёт timeout. Был определен захардкоженный лимит ровно в 20 секунд, который скорее всего оставили нам разработчики приложения bingo))
Т.к. мы не можем повлиять на скорость отдачи запросов сервисом оптимизировав и правив бинарник, то в голову пришла единственная идея - это оптимизация запросов от сервиса в базе данных. Посмотрев в логи БД я увидел такого рода запрос:

```sql
SELECT sessions.id, sessions.start_time, customers.id, customers.name, customers.surname, customers.birthday, customers.email, movies.id, movies.name, movies.year, movies.duration FROM sessions INNER JOIN customers ON sessions.customer_id = customers.id INNER JOIN movies ON sessions.movie_id = movies.id ORDER BY movies.year DESC, movies.name ASC, customers.id, sessions.id DESC LIMIT 100000
```
Здесь мы видим поля, которые потенциально очень хорошо можно ускорить создав для них индексы.

```sql
CREATE INDEX idx_sessions_customer_id ON sessions (customer_id);

CREATE INDEX idx_sessions_movie_id ON sessions (movie_id);

CREATE INDEX idx_movies_year ON movies (year);

CREATE INDEX idx_movies_name ON movies (name);

CREATE INDEX idx_customers_id ON customers (id);

CREATE INDEX idx_movies_id ON movies (id);
```
Наш неоптимизрованный запрос теперь не будет тратить лишние проходки при выполнении.

Результат оптимизации:

Время выполнения запроса из лога без оптимизации ~2,5 минуты.

С индексами примерно ~7 секунд, а это значит что теперь мы укладываемся в 20 секунд таймаута в приложении.

## Out of memory

Еще локально начиная разворачивать и щупать бинарный файл, я начал замечать, что он начинает просто банально класть мою линуксовую виртуалку. Определить сбой мне не удавалось. Только после того, как я начал устанавливать всё в облако я заметил такую особенность, что если оставить инфраструктуру развернутой на пару часов, то сами виртуалки на которых крутилось приложение падали без возможности авторестарта. Только уже после какого-то времени я увидел в `docker ps` статус контейнера `Exited (137)`. Погуглив я понял, что это проблема в памяти и контейнер позволял себе есть всю память виртуалки и мой сервис прибивался OOMKiller'ом. Решением было добавить лимит по памяти в docker-compose с приложением. Теперь докер сам будет прибивать контейнер при превышении лимита и рестартить его автоматически, не убивая всю ноду.

## Пункт "Оптимизация 3"

С этим я бадался наверное дня два, пытаясь придумать грамотный скрипт. Проблема в том, что моя установка после тестов Пети просто ложилась без автовосстановления. Это выглядело примерно так: тесты прошли, балансировщик определяет обе ноды как "UNHEALTHY", я захожу на любую из двух виртуалок где сам сервис лежит без реакции.
На localhost/ping отдаётся загадочная надпись 'I feel bad'. Был определен код ошибки HTTP 500. Понимая, что скорее всего это поведение, которое нельзя предугадать, то было принято решение сделать авторестарт на баш скрипте в случае такого кода ответа. Рабочий скрипт лежит в каталоге `/bingo` и еще один неудачный вариант на память я положил в `/materials/examples`

## Потенциальные проблемы

Потенцильные проблемы, которые не были решены

* Не решена проблема с тем, что при terraform apply обе одновременно поднятые ноды могут начать запускать prepare_db вместе.
* В конфигурационных файлах айпишники могут меняться. В моей установке использовался 1 статический айпишник для балансировщика нагрузки (инсталляция проводилась на бесплатном гранте от Тренировок, в которой есть квота на 2 статических айпишника, еще один он куда-то дел, я так и не разобрался)
* Все IP-адреса машин торчат в интернет, включая балансировщик, что потенциально сказывается на безопасности сервиса. Я не успел спрятать все машины под внутреннюю сеть, а в идеале вместо load balancer инстанса от яндекса нужно было использовать одну машину с nginx'ом